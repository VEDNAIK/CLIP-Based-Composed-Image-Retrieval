{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class Criterion(nn.Module):\n",
    "    \"\"\"\n",
    "    Batch-based classifcation loss\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Criterion, self).__init__()\n",
    "    \n",
    "    def forward(self, scores):\n",
    "        return F.cross_entropy(\n",
    "            scores, \n",
    "            torch.arange(scores.shape[0]).long().to(scores.device)\n",
    "        )\n",
    "\n",
    "\n",
    "class Combiner(nn.Module):\n",
    "    \"\"\" TODO: Combiner module, which fuses textual and visual information.\n",
    "    Given an image feature and a text feature, you should fuse them to get a fused feature. The dimension of the fused feature should be embed_dim.\n",
    "    Hint: You can concatenate image and text features and feed them to a FC layer, or you can devise your own fusion module, e.g., add, multiply, or attention, to achieve a higher retrieval score.\n",
    "    \"\"\"\n",
    "    def __init__(self, vision_feature_dim, text_feature_dim, embed_dim):\n",
    "        super(Combiner, self).__init__()\n",
    "        # Fully connected layer to project concatenated features to embed_dim\n",
    "        self.fc = nn.Linear(vision_feature_dim + text_feature_dim, embed_dim)\n",
    "\n",
    "    def forward(self, image_features, text_features):\n",
    "        # Concatenate image and text features\n",
    "        combined_features = torch.cat((image_features, text_features), dim=-1)\n",
    "        # Pass through fully connected layer\n",
    "        fused_features = self.fc(combined_features)\n",
    "        return fused_features\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    CLIP-based Composed Image Retrieval Model.\n",
    "    \"\"\"\n",
    "    def __init__(self, vision_feature_dim, text_feature_dim, embed_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.vision_feature_dim = vision_feature_dim\n",
    "        self.text_feature_dim = text_feature_dim\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Load clip model and freeze its parameters\n",
    "        self.clip_model, self.preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "        for p in self.clip_model.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "        self.combiner = Combiner(vision_feature_dim, text_feature_dim, embed_dim)\n",
    "    \n",
    "    def train(self):\n",
    "        self.combiner.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.combiner.eval()\n",
    "    \n",
    "    def encode_image(self, image_paths):\n",
    "        \"\"\" TODO: Encode images to get image features by the vision encoder of clip model. See https://github.com/openai/CLIP\n",
    "        Note: The clip model has loaded in the __init__() function. You do not need to create and load it on your own.\n",
    "\n",
    "        Args:\n",
    "            Image_paths (list[str]): a list of image paths.\n",
    "        \n",
    "        Returns:\n",
    "            vision_features (torch.Tensor): image features.\n",
    "        \"\"\"\n",
    "        images = [self.preprocess(Image.open(path)).unsqueeze(0) for path in image_paths]\n",
    "        images = torch.cat(images).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            vision_features = self.clip_model.encode_image(images)\n",
    "        \n",
    "        return vision_features.float() # Convert to float32 data type\n",
    "\n",
    "    def encode_text(self, texts):\n",
    "        \"\"\" TODO: Encode texts to get text features by the text encoder of clip model. See https://github.com/openai/CLIP\n",
    "        Note: The clip model has loaded in the __init__() function. You do not need to create and load it on your own.\n",
    "\n",
    "        Args:\n",
    "            texts (list[str]): a list of captions.\n",
    "        \n",
    "        Returns:\n",
    "            text_features (torch.Tensor): text features.\n",
    "        \"\"\"\n",
    "        tokens = clip.tokenize(texts).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_features = self.clip_model.encode_text(tokens)\n",
    "\n",
    "        return text_features.float()# Convert to float32 data type\n",
    "\n",
    "    def inference(self, ref_image_paths, texts):\n",
    "        with torch.no_grad():\n",
    "            ref_vision_features = self.encode_image(ref_image_paths)\n",
    "            text_features = self.encode_text(texts)\n",
    "            fused_features = self.combiner(ref_vision_features, text_features)\n",
    "        return fused_features\n",
    "    \n",
    "    def forward(self, ref_image_paths, texts, tgt_image_paths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ref_image_paths (list[str]): image paths of reference images.\n",
    "            texts (list[str]): captions.\n",
    "            tgt_image_paths (list[str]): image paths of reference images.\n",
    "        \n",
    "        Returns:\n",
    "            scores (torch.Tensor): score matrix with shape batch_size * batch_size.\n",
    "        \"\"\"\n",
    "        batch_size = len(ref_image_paths)\n",
    "\n",
    "        # Extract vision and text features\n",
    "        with torch.no_grad():\n",
    "            ref_vision_features = self.encode_image(ref_image_paths)\n",
    "            tgt_vision_features = self.encode_image(tgt_image_paths)\n",
    "            text_features = self.encode_text(texts)\n",
    "        assert ref_vision_features.shape == torch.Size([batch_size, self.vision_feature_dim])\n",
    "        assert tgt_vision_features.shape == torch.Size([batch_size, self.vision_feature_dim])\n",
    "        assert text_features.shape == torch.Size([batch_size, self.text_feature_dim])\n",
    "\n",
    "        # Fuse vision and text features \n",
    "        fused_features = self.combiner(ref_vision_features, text_features)\n",
    "        assert fused_features.shape == torch.Size([batch_size, self.embed_dim])\n",
    "\n",
    "        # L2 norm\n",
    "        fused_features = F.normalize(fused_features)\n",
    "        tgt_vision_features = F.normalize(tgt_vision_features)\n",
    "\n",
    "        # Calculate scores\n",
    "        scores = self.temperature.exp() * fused_features @ tgt_vision_features.t()\n",
    "        assert scores.shape == torch.Size([batch_size, batch_size])\n",
    "\n",
    "        return scores\n",
    "\n",
    "# Training function\n",
    "def train(data_loader, model, criterion, optimizer, log_step=15):\n",
    "    model.train()\n",
    "    for i, (_, ref_img_paths, tgt_img_paths, raw_captions) in enumerate(data_loader):\n",
    "        # Clear the previous gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the model\n",
    "        scores = model(ref_img_paths, raw_captions, tgt_img_paths)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(scores)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer step to update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log the training loss every log_step\n",
    "        if i % log_step == 0:\n",
    "            print(\"training loss: {:.3f}\".format(loss.item()))\n",
    "            \n",
    "# Validation function\n",
    "def eval_batch(data_loader, model, ranker):\n",
    "    model.eval()\n",
    "    ranker.update_emb(model)\n",
    "    rankings = []\n",
    "    for meta_info, ref_img_paths, _, raw_captions in data_loader:\n",
    "        with torch.no_grad():\n",
    "            fused_features = model.inference(ref_img_paths, raw_captions)\n",
    "            target_asins = [ meta_info[m]['target'] for m in range(len(meta_info)) ]\n",
    "            rankings.append(ranker.compute_rank(fused_features, target_asins))\n",
    "    metrics = {}\n",
    "    rankings = torch.cat(rankings, dim=0)\n",
    "    metrics['score'] = 1 - rankings.mean().item() / ranker.data_emb.size(0)\n",
    "    model.train()\n",
    "    return metrics\n",
    "\n",
    "def val(data_loader, model, ranker, best_score):\n",
    "    model.eval()\n",
    "    metrics = eval_batch(data_loader, model, ranker)\n",
    "    dev_score = metrics['score']\n",
    "    best_score = max(best_score, dev_score)\n",
    "    print('-' * 77)\n",
    "    print('| score {:8.5f} / {:8.5f} '.format(dev_score, best_score))\n",
    "    print('-' * 77)\n",
    "    print('best_dev_score: {}'.format(best_score))\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_custom_input(model, ref_image_paths, texts, tgt_image_paths=None):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Encode reference images and text\n",
    "        ref_vision_features = model.encode_image(ref_image_paths)\n",
    "        text_features = model.encode_text(texts)\n",
    "        \n",
    "        if tgt_image_paths is not None:\n",
    "            tgt_vision_features = model.encode_image(tgt_image_paths)\n",
    "            fused_features = model.combiner(ref_vision_features, text_features)\n",
    "            fused_features = F.normalize(fused_features)\n",
    "            tgt_vision_features = F.normalize(tgt_vision_features)\n",
    "            # Calculate similarity score\n",
    "            scores = model.temperature.exp() * fused_features @ tgt_vision_features.t()\n",
    "            return scores\n",
    "        else:\n",
    "            # Only return fused features if no target images are provided\n",
    "            fused_features = model.combiner(ref_vision_features, text_features)\n",
    "            return fused_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(vision_feature_dim, text_feature_dim, embed_dim, model_path):\n",
    "    model = Model(vision_feature_dim, text_feature_dim, embed_dim)\n",
    "    model.load_state_dict(torch.load(\"/Users/ved/Desktop/Sem 1/Vision and Language/Fall24_CSE597_Homework1/CIR/trained_model.pth\", map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mresize_images\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resize_images_parallel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Define input directories and output directories for resized images\u001b[39;00m\n\u001b[1;32m      4\u001b[0m input_ref_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ved/Desktop/Sem 1/Vision and Language/Fall24_CSE597_Homework1/inputref\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Directory containing your reference images\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Sem 1/Vision and Language/Fall24_CSE597_Homework1/CIR/resize_images.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parallel, delayed\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresize_image\u001b[39m(image, size):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'joblib'"
     ]
    }
   ],
   "source": [
    "from resize_images import resize_images_parallel\n",
    "\n",
    "# Define input directories and output directories for resized images\n",
    "input_ref_dir = \"/Users/ved/Desktop/Sem 1/Vision and Language/Fall24_CSE597_Homework1/inputref\"  # Directory containing your reference images\n",
    "input_tgt_dir = \"/Users/ved/Desktop/Sem 1/Vision and Language/Fall24_CSE597_Homework1/outputref\"  # Directory containing your target images\n",
    "output_ref_dir = \"\"  # Directory to save resized reference images\n",
    "output_tgt_dir = \"\"  # Directory to save resized target images\n",
    "image_size = 256  # Size to resize images to (256x256 in this case)\n",
    "\n",
    "# Resize reference images\n",
    "resize_images_parallel(input_ref_dir, output_ref_dir, (image_size, image_size))\n",
    "\n",
    "# Resize target images (if available)\n",
    "resize_images_parallel(input_tgt_dir, output_tgt_dir, (image_size, image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1m/v10vjxqd235fn30s91s6nnx40000gn/T/ipykernel_12843/2563257942.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/Users/ved/Desktop/Sem 1/Vision and Language/Fall24_CSE597_Homework1/CIR/trained_model.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted scores: tensor([[3.3151, 1.3381],\n",
      "        [0.9653, 1.4226]])\n"
     ]
    }
   ],
   "source": [
    "# Full process of prediction using custom input\n",
    "def main_predict():\n",
    "    # Define your parameters\n",
    "    vision_feature_dim = 512\n",
    "    text_feature_dim = 512\n",
    "    embed_dim = 512\n",
    "    model_path = \"path_to_your_trained_model.pth\"\n",
    "    \n",
    "    # Load your model\n",
    "    model = load_model(vision_feature_dim, text_feature_dim, embed_dim, 'CIR/trained_model.pth')\n",
    "\n",
    "    # Define custom input\n",
    "    ref_image_paths = [\"/Users/ved/Desktop/Sem 1/Vision and Language/Fall24_CSE597_Homework1/inputref/i1.png\", \"/Users/ved/Desktop/Sem 1/Vision and Language/Fall24_CSE597_Homework1/inputref/i2.png\"]\n",
    "    texts = [\"A man standing in front of a red car.\", \"A dog running on the beach during sunset.\"]\n",
    "    \n",
    "    # Optionally define target image paths if available\n",
    "    tgt_image_paths = [\"/Users/ved/Desktop/Sem 1/Vision and Language/Fall24_CSE597_Homework1/outputref/q1.png\", \"/Users/ved/Desktop/Sem 1/Vision and Language/Fall24_CSE597_Homework1/outputref/q2.png\"]\n",
    "\n",
    "    # Predict scores between reference and target images\n",
    "    scores = predict_custom_input(model, ref_image_paths, texts, tgt_image_paths)\n",
    "    \n",
    "    print(\"Predicted scores:\", scores)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
